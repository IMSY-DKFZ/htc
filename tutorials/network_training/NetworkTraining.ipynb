{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7afd40d-4a68-4278-be59-f9d0517b9c08",
   "metadata": {},
   "source": [
    "# Training a Network\n",
    "This notebook will guide you through the task of training a segmentation network within the htc framework. This is a complex task and a full training setup with all the details would be too much for a single tutorial. Hence, we consider an artificial simple example of segmenting only heart and lung in images from all annotators and we also don't use the full dataset but only a reduced version to keep the training time short.\n",
    "\n",
    "In summary, we need the following components to solve this training task:\n",
    "- Data specification which defines our training setup (folds and splits).\n",
    "- A lightning class where we define the model and our training/validation.\n",
    "- A configuration file with all our training settings.\n",
    "- Training of the network.\n",
    "- Aggregating the fold results and visualizing the results.\n",
    "- At the very end of our experiments: compute the results on the (untouched) test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0834bae-acbf-4e36-ad79-888b4cdd13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import JSON\n",
    "\n",
    "from htc import (\n",
    "    Config,\n",
    "    DataPath,\n",
    "    DataSpecification,\n",
    "    MetricAggregation,\n",
    "    SpecsGeneration,\n",
    "    create_class_scores_figure,\n",
    "    settings,\n",
    ")\n",
    "\n",
    "tutorial_dir = Path().absolute()\n",
    "assert (tutorial_dir / \"LightningImageThoracic.py\").exists(), (\n",
    "    \"Cannot find the network_training directory. Please make sure that the working directory contains this notebook\"\n",
    "    f\" (current working directory: {tutorial_dir})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef18d3c-d9f1-4d31-a2d7-7fd840602452",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Specification\n",
    "The data specification is a fundamental concept and is required for any training task. Here you define which images should be loaded in which fold/split. Since we deal with hierarchical data (multiple images per subject), we usually make cuts on the subject and not on the image level, i.e. we put all  images from the same subject into one split.\n",
    "\n",
    "To keep it simple, we use only 2 folds with three splits each (train, validation and test). We will use images from 3 pigs in total. For the generation of the specs file, we can use the `SpecsGeneration` class and here we only need to overwrite the `generate_folds()` method which returns a list of dicts with each dict defining one fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b08edd0-35b1-423e-939b-c17b7a9e8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecsGenerationThoracic(SpecsGeneration):\n",
    "    def __init__(self):\n",
    "        # Unique name of the resulting specs file\n",
    "        super().__init__(name=\"pigs_thoracic_2folds\")\n",
    "\n",
    "        # We will divide the data into an untuched test set and the rest will be divided into 2 folds\n",
    "        # Here, we are doing leave-one-subject-out cross validation with two subjects and have an additional subject in the untouched test set\n",
    "        self.subjects_folds = [\"P093\", \"P094\"]\n",
    "        self.subjects_test = [\"P095\"]\n",
    "\n",
    "    def generate_folds(self) -> list[dict]:\n",
    "        # We only need a subset of the available images: those from our selected subjects (filter_subjects_*) and here only the images which contain our labels (filter_labels)\n",
    "        data_dir = settings.data_dirs[\"HeiPorSPECTRAL\"]\n",
    "        filter_labels = lambda p: set(p.annotated_labels(annotation_name=\"all\")) & {\"heart\", \"lung\"}\n",
    "        filter_subjects_test = lambda p: p.subject_name in self.subjects_test\n",
    "        filter_subjects_folds = lambda p: p.subject_name in self.subjects_folds\n",
    "\n",
    "        # Untouched test set\n",
    "        paths_test = list(DataPath.iterate(data_dir, filters=[filter_subjects_test, filter_labels]))\n",
    "        imgs_test = [p.image_name() for p in paths_test]\n",
    "\n",
    "        # Fold paths\n",
    "        paths_folds = list(DataPath.iterate(data_dir, filters=[filter_subjects_folds, filter_labels]))\n",
    "\n",
    "        data_specs = []\n",
    "        for fold_subject in self.subjects_folds:\n",
    "            # Divide the folds into train and validation\n",
    "            imgs_train = [p.image_name() for p in paths_folds if p.subject_name == fold_subject]\n",
    "            imgs_val = [p.image_name() for p in paths_folds if p.subject_name != fold_subject]\n",
    "\n",
    "            fold_specs = {\n",
    "                \"fold_name\": f\"fold_{fold_subject}\",\n",
    "                \"train\": {\n",
    "                    \"image_names\": imgs_train,\n",
    "                },\n",
    "                \"val\": {\n",
    "                    \"image_names\": imgs_val,\n",
    "                },\n",
    "                \"test\": {\n",
    "                    \"image_names\": imgs_test,\n",
    "                },\n",
    "            }\n",
    "\n",
    "            data_specs.append(fold_specs)\n",
    "\n",
    "        return data_specs\n",
    "\n",
    "\n",
    "SpecsGenerationThoracic().generate_dataset(target_folder=tutorial_dir)\n",
    "specs_path = tutorial_dir / \"pigs_thoracic_2folds.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd43f358-bede-4f81-9a7d-92ecc0a43032",
   "metadata": {},
   "source": [
    "When you exectue the above cell, a JSON file will be created in the folder of this notebook. This file defines our two folds and the subject which is used for training (`P093` in the first and `P094` in the second fold) and validation (`P094` in the first and `P093` in the second fold). For each split, we simply list the images which belong to this split. `P095` is used as test set for all folds The test set must remain untouched until all experiments are complete and we are happy with the performance on the validation set. Only then is it time to take a look at the test set.\n",
    "> The list of image names is as flexible as the `DataPath.from_image_name()` function. This means you can also specify the annotations you want to use on a per image basis, i.e. `image_name@name1&name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61131646-6607-42e0-85ee-965cc5450593",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "fold_name": "fold_P093",
        "test": {
         "image_names": [
          "P095#2021_05_02_19_34_43",
          "P095#2021_05_02_19_35_01",
          "P095#2021_05_02_19_35_18",
          "P095#2021_05_02_19_36_16",
          "P095#2021_05_02_19_36_34",
          "P095#2021_05_02_19_36_51",
          "P095#2021_05_02_19_37_28",
          "P095#2021_05_02_19_37_46",
          "P095#2021_05_02_19_38_03",
          "P095#2021_05_02_19_38_36",
          "P095#2021_05_02_19_38_54",
          "P095#2021_05_02_19_39_11",
          "P095#2021_05_02_19_40_06",
          "P095#2021_05_02_19_40_23",
          "P095#2021_05_02_19_40_40",
          "P095#2021_05_02_19_41_13",
          "P095#2021_05_02_19_41_31",
          "P095#2021_05_02_19_41_48",
          "P095#2021_05_02_19_42_42",
          "P095#2021_05_02_19_43_00",
          "P095#2021_05_02_19_43_17",
          "P095#2021_05_02_19_43_43",
          "P095#2021_05_02_19_44_00",
          "P095#2021_05_02_19_44_17",
          "P095#2021_05_02_19_44_47",
          "P095#2021_05_02_19_45_06",
          "P095#2021_05_02_19_45_23",
          "P095#2021_05_02_19_46_28",
          "P095#2021_05_02_19_46_46",
          "P095#2021_05_02_19_47_03",
          "P095#2021_05_02_19_47_38",
          "P095#2021_05_02_19_47_55",
          "P095#2021_05_02_19_48_12",
          "P095#2021_05_02_19_48_44",
          "P095#2021_05_02_19_49_01",
          "P095#2021_05_02_19_49_18",
          "P095#2021_05_02_19_50_01",
          "P095#2021_05_02_19_50_19",
          "P095#2021_05_02_19_50_36",
          "P095#2021_05_02_19_51_16",
          "P095#2021_05_02_19_51_33",
          "P095#2021_05_02_19_51_52",
          "P095#2021_05_02_19_52_21",
          "P095#2021_05_02_19_52_38",
          "P095#2021_05_02_19_52_56",
          "P095#2021_05_02_19_53_36",
          "P095#2021_05_02_19_53_53",
          "P095#2021_05_02_19_54_11",
          "P095#2021_05_02_19_54_44",
          "P095#2021_05_02_19_55_01",
          "P095#2021_05_02_19_55_19",
          "P095#2021_05_02_19_55_59",
          "P095#2021_05_02_19_56_17",
          "P095#2021_05_02_19_56_34",
          "P095#2021_05_02_19_57_04",
          "P095#2021_05_02_19_57_22",
          "P095#2021_05_02_19_57_40",
          "P095#2021_05_02_19_58_04",
          "P095#2021_05_02_19_58_22",
          "P095#2021_05_02_19_58_39",
          "P095#2021_05_02_19_59_17",
          "P095#2021_05_02_19_59_34",
          "P095#2021_05_02_19_59_52",
          "P095#2021_05_02_20_00_18",
          "P095#2021_05_02_20_00_36",
          "P095#2021_05_02_20_00_54",
          "P095#2021_05_02_20_01_39",
          "P095#2021_05_02_20_01_56",
          "P095#2021_05_02_20_02_13",
          "P095#2021_05_02_20_02_43",
          "P095#2021_05_02_20_03_00",
          "P095#2021_05_02_20_03_17"
         ]
        },
        "train": {
         "image_names": [
          "P093#2021_04_28_15_30_33",
          "P093#2021_04_28_15_30_50",
          "P093#2021_04_28_15_31_07",
          "P093#2021_04_28_15_31_48",
          "P093#2021_04_28_15_32_06",
          "P093#2021_04_28_15_32_23",
          "P093#2021_04_28_15_32_55",
          "P093#2021_04_28_15_33_11",
          "P093#2021_04_28_15_33_28",
          "P093#2021_04_28_15_34_17",
          "P093#2021_04_28_15_34_34",
          "P093#2021_04_28_15_34_51",
          "P093#2021_04_28_15_35_19",
          "P093#2021_04_28_15_35_37",
          "P093#2021_04_28_15_35_56",
          "P093#2021_04_28_15_36_45",
          "P093#2021_04_28_15_37_02",
          "P093#2021_04_28_15_37_18",
          "P093#2021_04_28_15_37_57",
          "P093#2021_04_28_15_38_14",
          "P093#2021_04_28_15_38_31",
          "P093#2021_04_28_15_39_04",
          "P093#2021_04_28_15_39_22",
          "P093#2021_04_28_15_39_39",
          "P093#2021_04_28_15_40_13",
          "P093#2021_04_28_15_40_30",
          "P093#2021_04_28_15_40_46",
          "P093#2021_04_28_15_41_45",
          "P093#2021_04_28_15_42_03",
          "P093#2021_04_28_15_42_19",
          "P093#2021_04_28_15_43_11",
          "P093#2021_04_28_15_43_28",
          "P093#2021_04_28_15_43_44",
          "P093#2021_04_28_15_44_27",
          "P093#2021_04_28_15_44_44",
          "P093#2021_04_28_15_45_00",
          "P093#2021_04_28_15_45_43",
          "P093#2021_04_28_15_46_01",
          "P093#2021_04_28_15_46_18",
          "P093#2021_04_28_15_46_48",
          "P093#2021_04_28_15_47_06",
          "P093#2021_04_28_15_47_22",
          "P093#2021_04_28_15_47_53",
          "P093#2021_04_28_15_48_09",
          "P093#2021_04_28_15_48_27",
          "P093#2021_04_28_15_49_05",
          "P093#2021_04_28_15_49_22",
          "P093#2021_04_28_15_49_38",
          "P093#2021_04_28_15_50_30",
          "P093#2021_04_28_15_50_47",
          "P093#2021_04_28_15_51_24",
          "P093#2021_04_28_15_52_00",
          "P093#2021_04_28_15_52_17",
          "P093#2021_04_28_15_52_33",
          "P093#2021_04_28_15_53_19",
          "P093#2021_04_28_15_53_37",
          "P093#2021_04_28_15_53_53",
          "P093#2021_04_28_15_54_31",
          "P093#2021_04_28_15_54_48",
          "P093#2021_04_28_15_55_05",
          "P093#2021_04_28_15_55_35",
          "P093#2021_04_28_15_55_53",
          "P093#2021_04_28_15_56_09",
          "P093#2021_04_28_15_56_57",
          "P093#2021_04_28_15_57_14",
          "P093#2021_04_28_15_57_31",
          "P093#2021_04_28_15_58_01",
          "P093#2021_04_28_15_58_20",
          "P093#2021_04_28_15_58_37",
          "P093#2021_04_28_15_59_13",
          "P093#2021_04_28_15_59_31",
          "P093#2021_04_28_15_59_47"
         ]
        },
        "val": {
         "image_names": [
          "P094#2021_04_30_17_10_23",
          "P094#2021_04_30_17_10_40",
          "P094#2021_04_30_17_10_57",
          "P094#2021_04_30_17_11_25",
          "P094#2021_04_30_17_11_42",
          "P094#2021_04_30_17_12_00",
          "P094#2021_04_30_17_12_35",
          "P094#2021_04_30_17_12_52",
          "P094#2021_04_30_17_13_10",
          "P094#2021_04_30_17_13_43",
          "P094#2021_04_30_17_14_00",
          "P094#2021_04_30_17_14_17",
          "P094#2021_04_30_17_14_42",
          "P094#2021_04_30_17_14_59",
          "P094#2021_04_30_17_15_16",
          "P094#2021_04_30_17_15_45",
          "P094#2021_04_30_17_16_02",
          "P094#2021_04_30_17_16_19",
          "P094#2021_04_30_17_16_55",
          "P094#2021_04_30_17_17_12",
          "P094#2021_04_30_17_17_30",
          "P094#2021_04_30_17_17_54",
          "P094#2021_04_30_17_18_11",
          "P094#2021_04_30_17_18_27",
          "P094#2021_04_30_17_19_21",
          "P094#2021_04_30_17_19_37",
          "P094#2021_04_30_17_19_54",
          "P094#2021_04_30_17_20_35",
          "P094#2021_04_30_17_20_52",
          "P094#2021_04_30_17_21_09",
          "P094#2021_04_30_17_21_40",
          "P094#2021_04_30_17_21_56",
          "P094#2021_04_30_17_22_13",
          "P094#2021_04_30_17_22_48",
          "P094#2021_04_30_17_23_05",
          "P094#2021_04_30_17_23_22",
          "P094#2021_04_30_17_24_33",
          "P094#2021_04_30_17_24_50",
          "P094#2021_04_30_17_25_07",
          "P094#2021_04_30_17_25_37",
          "P094#2021_04_30_17_25_54",
          "P094#2021_04_30_17_26_11",
          "P094#2021_04_30_17_26_51",
          "P094#2021_04_30_17_27_08",
          "P094#2021_04_30_17_27_25",
          "P094#2021_04_30_17_28_37",
          "P094#2021_04_30_17_28_54",
          "P094#2021_04_30_17_29_11",
          "P094#2021_04_30_17_29_55",
          "P094#2021_04_30_17_30_12",
          "P094#2021_04_30_17_30_28",
          "P094#2021_04_30_17_31_02",
          "P094#2021_04_30_17_31_19",
          "P094#2021_04_30_17_31_36",
          "P094#2021_04_30_17_32_27",
          "P094#2021_04_30_17_32_44",
          "P094#2021_04_30_17_33_01",
          "P094#2021_04_30_17_33_29",
          "P094#2021_04_30_17_33_46",
          "P094#2021_04_30_17_34_03",
          "P094#2021_04_30_17_34_42",
          "P094#2021_04_30_17_35_00",
          "P094#2021_04_30_17_35_17",
          "P094#2021_04_30_17_36_00",
          "P094#2021_04_30_17_36_17",
          "P094#2021_04_30_17_36_34",
          "P094#2021_04_30_17_37_08",
          "P094#2021_04_30_17_37_25",
          "P094#2021_04_30_17_37_43",
          "P094#2021_04_30_17_38_19",
          "P094#2021_04_30_17_38_36",
          "P094#2021_04_30_17_38_53"
         ]
        }
       },
       {
        "fold_name": "fold_P094",
        "test": {
         "image_names": [
          "P095#2021_05_02_19_34_43",
          "P095#2021_05_02_19_35_01",
          "P095#2021_05_02_19_35_18",
          "P095#2021_05_02_19_36_16",
          "P095#2021_05_02_19_36_34",
          "P095#2021_05_02_19_36_51",
          "P095#2021_05_02_19_37_28",
          "P095#2021_05_02_19_37_46",
          "P095#2021_05_02_19_38_03",
          "P095#2021_05_02_19_38_36",
          "P095#2021_05_02_19_38_54",
          "P095#2021_05_02_19_39_11",
          "P095#2021_05_02_19_40_06",
          "P095#2021_05_02_19_40_23",
          "P095#2021_05_02_19_40_40",
          "P095#2021_05_02_19_41_13",
          "P095#2021_05_02_19_41_31",
          "P095#2021_05_02_19_41_48",
          "P095#2021_05_02_19_42_42",
          "P095#2021_05_02_19_43_00",
          "P095#2021_05_02_19_43_17",
          "P095#2021_05_02_19_43_43",
          "P095#2021_05_02_19_44_00",
          "P095#2021_05_02_19_44_17",
          "P095#2021_05_02_19_44_47",
          "P095#2021_05_02_19_45_06",
          "P095#2021_05_02_19_45_23",
          "P095#2021_05_02_19_46_28",
          "P095#2021_05_02_19_46_46",
          "P095#2021_05_02_19_47_03",
          "P095#2021_05_02_19_47_38",
          "P095#2021_05_02_19_47_55",
          "P095#2021_05_02_19_48_12",
          "P095#2021_05_02_19_48_44",
          "P095#2021_05_02_19_49_01",
          "P095#2021_05_02_19_49_18",
          "P095#2021_05_02_19_50_01",
          "P095#2021_05_02_19_50_19",
          "P095#2021_05_02_19_50_36",
          "P095#2021_05_02_19_51_16",
          "P095#2021_05_02_19_51_33",
          "P095#2021_05_02_19_51_52",
          "P095#2021_05_02_19_52_21",
          "P095#2021_05_02_19_52_38",
          "P095#2021_05_02_19_52_56",
          "P095#2021_05_02_19_53_36",
          "P095#2021_05_02_19_53_53",
          "P095#2021_05_02_19_54_11",
          "P095#2021_05_02_19_54_44",
          "P095#2021_05_02_19_55_01",
          "P095#2021_05_02_19_55_19",
          "P095#2021_05_02_19_55_59",
          "P095#2021_05_02_19_56_17",
          "P095#2021_05_02_19_56_34",
          "P095#2021_05_02_19_57_04",
          "P095#2021_05_02_19_57_22",
          "P095#2021_05_02_19_57_40",
          "P095#2021_05_02_19_58_04",
          "P095#2021_05_02_19_58_22",
          "P095#2021_05_02_19_58_39",
          "P095#2021_05_02_19_59_17",
          "P095#2021_05_02_19_59_34",
          "P095#2021_05_02_19_59_52",
          "P095#2021_05_02_20_00_18",
          "P095#2021_05_02_20_00_36",
          "P095#2021_05_02_20_00_54",
          "P095#2021_05_02_20_01_39",
          "P095#2021_05_02_20_01_56",
          "P095#2021_05_02_20_02_13",
          "P095#2021_05_02_20_02_43",
          "P095#2021_05_02_20_03_00",
          "P095#2021_05_02_20_03_17"
         ]
        },
        "train": {
         "image_names": [
          "P094#2021_04_30_17_10_23",
          "P094#2021_04_30_17_10_40",
          "P094#2021_04_30_17_10_57",
          "P094#2021_04_30_17_11_25",
          "P094#2021_04_30_17_11_42",
          "P094#2021_04_30_17_12_00",
          "P094#2021_04_30_17_12_35",
          "P094#2021_04_30_17_12_52",
          "P094#2021_04_30_17_13_10",
          "P094#2021_04_30_17_13_43",
          "P094#2021_04_30_17_14_00",
          "P094#2021_04_30_17_14_17",
          "P094#2021_04_30_17_14_42",
          "P094#2021_04_30_17_14_59",
          "P094#2021_04_30_17_15_16",
          "P094#2021_04_30_17_15_45",
          "P094#2021_04_30_17_16_02",
          "P094#2021_04_30_17_16_19",
          "P094#2021_04_30_17_16_55",
          "P094#2021_04_30_17_17_12",
          "P094#2021_04_30_17_17_30",
          "P094#2021_04_30_17_17_54",
          "P094#2021_04_30_17_18_11",
          "P094#2021_04_30_17_18_27",
          "P094#2021_04_30_17_19_21",
          "P094#2021_04_30_17_19_37",
          "P094#2021_04_30_17_19_54",
          "P094#2021_04_30_17_20_35",
          "P094#2021_04_30_17_20_52",
          "P094#2021_04_30_17_21_09",
          "P094#2021_04_30_17_21_40",
          "P094#2021_04_30_17_21_56",
          "P094#2021_04_30_17_22_13",
          "P094#2021_04_30_17_22_48",
          "P094#2021_04_30_17_23_05",
          "P094#2021_04_30_17_23_22",
          "P094#2021_04_30_17_24_33",
          "P094#2021_04_30_17_24_50",
          "P094#2021_04_30_17_25_07",
          "P094#2021_04_30_17_25_37",
          "P094#2021_04_30_17_25_54",
          "P094#2021_04_30_17_26_11",
          "P094#2021_04_30_17_26_51",
          "P094#2021_04_30_17_27_08",
          "P094#2021_04_30_17_27_25",
          "P094#2021_04_30_17_28_37",
          "P094#2021_04_30_17_28_54",
          "P094#2021_04_30_17_29_11",
          "P094#2021_04_30_17_29_55",
          "P094#2021_04_30_17_30_12",
          "P094#2021_04_30_17_30_28",
          "P094#2021_04_30_17_31_02",
          "P094#2021_04_30_17_31_19",
          "P094#2021_04_30_17_31_36",
          "P094#2021_04_30_17_32_27",
          "P094#2021_04_30_17_32_44",
          "P094#2021_04_30_17_33_01",
          "P094#2021_04_30_17_33_29",
          "P094#2021_04_30_17_33_46",
          "P094#2021_04_30_17_34_03",
          "P094#2021_04_30_17_34_42",
          "P094#2021_04_30_17_35_00",
          "P094#2021_04_30_17_35_17",
          "P094#2021_04_30_17_36_00",
          "P094#2021_04_30_17_36_17",
          "P094#2021_04_30_17_36_34",
          "P094#2021_04_30_17_37_08",
          "P094#2021_04_30_17_37_25",
          "P094#2021_04_30_17_37_43",
          "P094#2021_04_30_17_38_19",
          "P094#2021_04_30_17_38_36",
          "P094#2021_04_30_17_38_53"
         ]
        },
        "val": {
         "image_names": [
          "P093#2021_04_28_15_30_33",
          "P093#2021_04_28_15_30_50",
          "P093#2021_04_28_15_31_07",
          "P093#2021_04_28_15_31_48",
          "P093#2021_04_28_15_32_06",
          "P093#2021_04_28_15_32_23",
          "P093#2021_04_28_15_32_55",
          "P093#2021_04_28_15_33_11",
          "P093#2021_04_28_15_33_28",
          "P093#2021_04_28_15_34_17",
          "P093#2021_04_28_15_34_34",
          "P093#2021_04_28_15_34_51",
          "P093#2021_04_28_15_35_19",
          "P093#2021_04_28_15_35_37",
          "P093#2021_04_28_15_35_56",
          "P093#2021_04_28_15_36_45",
          "P093#2021_04_28_15_37_02",
          "P093#2021_04_28_15_37_18",
          "P093#2021_04_28_15_37_57",
          "P093#2021_04_28_15_38_14",
          "P093#2021_04_28_15_38_31",
          "P093#2021_04_28_15_39_04",
          "P093#2021_04_28_15_39_22",
          "P093#2021_04_28_15_39_39",
          "P093#2021_04_28_15_40_13",
          "P093#2021_04_28_15_40_30",
          "P093#2021_04_28_15_40_46",
          "P093#2021_04_28_15_41_45",
          "P093#2021_04_28_15_42_03",
          "P093#2021_04_28_15_42_19",
          "P093#2021_04_28_15_43_11",
          "P093#2021_04_28_15_43_28",
          "P093#2021_04_28_15_43_44",
          "P093#2021_04_28_15_44_27",
          "P093#2021_04_28_15_44_44",
          "P093#2021_04_28_15_45_00",
          "P093#2021_04_28_15_45_43",
          "P093#2021_04_28_15_46_01",
          "P093#2021_04_28_15_46_18",
          "P093#2021_04_28_15_46_48",
          "P093#2021_04_28_15_47_06",
          "P093#2021_04_28_15_47_22",
          "P093#2021_04_28_15_47_53",
          "P093#2021_04_28_15_48_09",
          "P093#2021_04_28_15_48_27",
          "P093#2021_04_28_15_49_05",
          "P093#2021_04_28_15_49_22",
          "P093#2021_04_28_15_49_38",
          "P093#2021_04_28_15_50_30",
          "P093#2021_04_28_15_50_47",
          "P093#2021_04_28_15_51_24",
          "P093#2021_04_28_15_52_00",
          "P093#2021_04_28_15_52_17",
          "P093#2021_04_28_15_52_33",
          "P093#2021_04_28_15_53_19",
          "P093#2021_04_28_15_53_37",
          "P093#2021_04_28_15_53_53",
          "P093#2021_04_28_15_54_31",
          "P093#2021_04_28_15_54_48",
          "P093#2021_04_28_15_55_05",
          "P093#2021_04_28_15_55_35",
          "P093#2021_04_28_15_55_53",
          "P093#2021_04_28_15_56_09",
          "P093#2021_04_28_15_56_57",
          "P093#2021_04_28_15_57_14",
          "P093#2021_04_28_15_57_31",
          "P093#2021_04_28_15_58_01",
          "P093#2021_04_28_15_58_20",
          "P093#2021_04_28_15_58_37",
          "P093#2021_04_28_15_59_13",
          "P093#2021_04_28_15_59_31",
          "P093#2021_04_28_15_59_47"
         ]
        }
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(specs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acef89a8-f82d-4ba3-8c69-0b40065d16f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold_name</th>\n",
       "      <th>split_name</th>\n",
       "      <th>image_name</th>\n",
       "      <th>subject_name</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fold_P093</td>\n",
       "      <td>train</td>\n",
       "      <td>P093#2021_04_28_15_30_33</td>\n",
       "      <td>P093</td>\n",
       "      <td>2021_04_28_15_30_33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fold_P093</td>\n",
       "      <td>train</td>\n",
       "      <td>P093#2021_04_28_15_30_50</td>\n",
       "      <td>P093</td>\n",
       "      <td>2021_04_28_15_30_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fold_P093</td>\n",
       "      <td>train</td>\n",
       "      <td>P093#2021_04_28_15_31_07</td>\n",
       "      <td>P093</td>\n",
       "      <td>2021_04_28_15_31_07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fold_P093</td>\n",
       "      <td>train</td>\n",
       "      <td>P093#2021_04_28_15_31_48</td>\n",
       "      <td>P093</td>\n",
       "      <td>2021_04_28_15_31_48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fold_P093</td>\n",
       "      <td>train</td>\n",
       "      <td>P093#2021_04_28_15_32_06</td>\n",
       "      <td>P093</td>\n",
       "      <td>2021_04_28_15_32_06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>fold_P094</td>\n",
       "      <td>val</td>\n",
       "      <td>P093#2021_04_28_15_58_20</td>\n",
       "      <td>P093</td>\n",
       "      <td>2021_04_28_15_58_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>fold_P094</td>\n",
       "      <td>val</td>\n",
       "      <td>P093#2021_04_28_15_58_37</td>\n",
       "      <td>P093</td>\n",
       "      <td>2021_04_28_15_58_37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>fold_P094</td>\n",
       "      <td>val</td>\n",
       "      <td>P093#2021_04_28_15_59_13</td>\n",
       "      <td>P093</td>\n",
       "      <td>2021_04_28_15_59_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>fold_P094</td>\n",
       "      <td>val</td>\n",
       "      <td>P093#2021_04_28_15_59_31</td>\n",
       "      <td>P093</td>\n",
       "      <td>2021_04_28_15_59_31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>fold_P094</td>\n",
       "      <td>val</td>\n",
       "      <td>P093#2021_04_28_15_59_47</td>\n",
       "      <td>P093</td>\n",
       "      <td>2021_04_28_15_59_47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     fold_name split_name                image_name subject_name  \\\n",
       "0    fold_P093      train  P093#2021_04_28_15_30_33         P093   \n",
       "1    fold_P093      train  P093#2021_04_28_15_30_50         P093   \n",
       "2    fold_P093      train  P093#2021_04_28_15_31_07         P093   \n",
       "3    fold_P093      train  P093#2021_04_28_15_31_48         P093   \n",
       "4    fold_P093      train  P093#2021_04_28_15_32_06         P093   \n",
       "..         ...        ...                       ...          ...   \n",
       "283  fold_P094        val  P093#2021_04_28_15_58_20         P093   \n",
       "284  fold_P094        val  P093#2021_04_28_15_58_37         P093   \n",
       "285  fold_P094        val  P093#2021_04_28_15_59_13         P093   \n",
       "286  fold_P094        val  P093#2021_04_28_15_59_31         P093   \n",
       "287  fold_P094        val  P093#2021_04_28_15_59_47         P093   \n",
       "\n",
       "               timestamp  \n",
       "0    2021_04_28_15_30_33  \n",
       "1    2021_04_28_15_30_50  \n",
       "2    2021_04_28_15_31_07  \n",
       "3    2021_04_28_15_31_48  \n",
       "4    2021_04_28_15_32_06  \n",
       "..                   ...  \n",
       "283  2021_04_28_15_58_20  \n",
       "284  2021_04_28_15_58_37  \n",
       "285  2021_04_28_15_59_13  \n",
       "286  2021_04_28_15_59_31  \n",
       "287  2021_04_28_15_59_47  \n",
       "\n",
       "[288 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also easily use this specs file in the htc framework\n",
    "DataSpecification(specs_path).table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8efd3f-1ed1-4515-90d3-6e4eff32fc03",
   "metadata": {},
   "source": [
    "## Lightning Class\n",
    "The lightning class defines our training based on a batch of images, i.e. the model we want to use, the computation of the loss or the validation of images. This is based on the [Lightning framework](https://www.pytorchlightning.ai/) with some additional extensions for our data. In our task, we want to predict segmentation masks on full images, so we can re-use a lot of the existing [`LightningImage`](../../htc/models/image/LightningImage.py) class which, for example, already computes the dice loss during training.\n",
    "\n",
    "Hence, in our own class, we inherit form [`LightningImage`](../../htc/models/image/LightningImage.py) and just overwrite the parts which we want to change.  For showcase purposes, we overwrite the `training_step()` method and add the [focal loss](https://docs.monai.io/en/latest/losses.html#focalloss) as additional loss (additional to the dice and cross entropy loss) for our training. In similar ways, you can basically overwrite any methods and adapt the training to your needs. You can find the class [`LightningImageThoracic`](LightningImageThoracic.py) in the same folder as this notebook.\n",
    "\n",
    "Please note that we also did not overwrite any validation or test methods (e.g. `validation_step()`). This is because LightningImage already inherits from [`EvaluationMixin`](../../htc/models/common/EvaluationMixin.py) which computes some default metrics like the dice score useful for segmentation tasks as ours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b13d01-0723-45fc-8ecd-e83ff7654547",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "The specification of your training is summarized in a configuration file. In the htc framework, we are using simple JSON files for this purpose. Here you define which data you want to use, the augmentations, etc. To make it simple, we inherit from the default configuration of the image model and just adapt the parts we need. These are:\n",
    "- Which lightning class do we want to use?\n",
    "- On which data do we want to train? (i.e. which specs file to use)\n",
    "- Which annotations to load?\n",
    "- How do we want to aggregate the metric scores? On the class-level or on the image-level? In the pretrained networks, we aggregated on the image-level yielding one score per subject. This makes only sense if the complete image is annotated and you want to know the segmentation performance for an image. Here, however, we do not have fully-segmented images, so an image-level score is less meaningful. Instead, we want to look at the class-level scores yielding a metric score per organ.\n",
    "- Which classes do we want to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e3ad0a-bc77-4fe8-84dd-c503730a7e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "config_name": "default",
       "dataloader_kwargs": {
        "batch_size": 5,
        "num_workers": 1
       },
       "inherits": "models/image/configs/default",
       "input": {
        "annotation_name": [
         "polygon#annotator1",
         "polygon#annotator2",
         "polygon#annotator3"
        ],
        "data_spec": "/mnt/ssd_8tb/htc/src/tutorials/network_training/pigs_thoracic_2folds.json",
        "epoch_size": 500,
        "merge_annotations": "union",
        "n_channels": 100,
        "preprocessing": "L1",
        "transforms_gpu": [
         {
          "class": "KorniaTransform",
          "degrees": 45,
          "p": 0.5,
          "padding_mode": "reflection",
          "scale": [
           0.9,
           1.1
          ],
          "transformation_name": "RandomAffine",
          "translate": [
           0.0625,
           0.0625
          ]
         },
         {
          "class": "KorniaTransform",
          "p": 0.25,
          "transformation_name": "RandomHorizontalFlip"
         },
         {
          "class": "KorniaTransform",
          "p": 0.25,
          "transformation_name": "RandomVerticalFlip"
         }
        ]
       },
       "label_mapping": {
        "heart": 0,
        "lung": 1,
        "unlabeled": 100
       },
       "lightning_class": "/mnt/ssd_8tb/htc/src/tutorials/network_training/LightningImageThoracic.py>LightningImageThoracic",
       "model": {
        "architecture_kwargs": {
         "encoder_name": "efficientnet-b5",
         "encoder_weights": "imagenet"
        },
        "architecture_name": "Unet",
        "model_name": "ModelImage"
       },
       "optimization": {
        "lr_scheduler": {
         "gamma": 0.99,
         "name": "ExponentialLR"
        },
        "optimizer": {
         "lr": 0.001,
         "name": "Adam",
         "weight_decay": 0
        }
       },
       "swa_kwargs": {
        "annealing_epochs": 0
       },
       "trainer_kwargs": {
        "accelerator": "gpu",
        "devices": 1,
        "enable_progress_bar": false,
        "max_epochs": 2,
        "precision": "16-mixed"
       },
       "validation": {
        "checkpoint_metric": "dice_metric",
        "checkpoint_metric_mode": "class_level",
        "dataset_index": 0
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config.from_model_name(\"default\", \"image\")\n",
    "config[\"inherits\"] = \"models/image/configs/default\"\n",
    "config[\"lightning_class\"] = tutorial_dir / \"LightningImageThoracic.py>LightningImageThoracic\"\n",
    "config[\"input/data_spec\"] = tutorial_dir / \"pigs_thoracic_2folds.json\"\n",
    "config[\"input/annotation_name\"] = [\"polygon#annotator1\", \"polygon#annotator2\", \"polygon#annotator3\"]\n",
    "config[\"validation/checkpoint_metric_mode\"] = \"class_level\"\n",
    "\n",
    "# We want to merge the annotations from all annotators into one label mask\n",
    "config[\"input/merge_annotations\"] = \"union\"\n",
    "\n",
    "# We have a two-class problem and we want to ignore all unlabeled pixels\n",
    "# Everything which is >= settings.label_index_thresh will later be considered invalid\n",
    "config[\"label_mapping\"] = {\n",
    "    \"heart\": 0,\n",
    "    \"lung\": 1,\n",
    "    \"unlabeled\": settings.label_index_thresh,\n",
    "}\n",
    "\n",
    "# Reduce the training time\n",
    "config[\"trainer_kwargs/max_epochs\"] = 2\n",
    "\n",
    "# Progress bars can cause problems in Jupyter notebooks so we disable them here (training does not take super long)\n",
    "config[\"trainer_kwargs/enable_progress_bar\"] = False\n",
    "\n",
    "# Uncomment the following lines if you want to use one of the pretrained models as basis for our training\n",
    "# config[\"model/pretrained_model\"] = {\n",
    "#     \"model\": \"image\",\n",
    "#     \"run_folder\": \"2022-02-03_22-58-44_generated_default_model_comparison\",\n",
    "# }\n",
    "\n",
    "config_path = tutorial_dir / \"config_thoracic.json\"\n",
    "config.save_config(config_path)\n",
    "JSON(config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed9af8-003f-4ebe-8016-bb88bacd46c3",
   "metadata": {},
   "source": [
    "## Start the Training\n",
    "We now have everything together to start the training of our network. Simply run the `htc training` command and pass the modle type (image model in our case) and path to the config as arguments.\n",
    "> &#x26a0;&#xfe0f; Starting a training session in a Jupyter notebook is usually not a good idea. Instead, it is advisable to use a [`screen`](https://linuxize.com/post/how-to-use-linux-screen/) environment so that your training runs in the background and you can return later to check for the status.\n",
    "\n",
    "> There is also a `--fold FOLD_NAME` switch if you only want to train only one fold. This is useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b656586-026a-4153-a2c5-f6871c6ec18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Starting training of the fold fold_P093 \u001b[1m[\u001b[0m\u001b[37m1\u001b[0m/\u001b[37m2\u001b[0m\u001b[1m]\u001b[0m    \u001b[2mrun_training.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m265\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The following config will be used for training:   \u001b[2mrun_training.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m70\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m \u001b[1m{\u001b[0m\u001b[90m'config_name'\u001b[0m: \u001b[90m'config_thoracic'\u001b[0m,                \u001b[2mrun_training.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m71\u001b[0m\n",
      " \u001b[90m'dataloader_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'batch_size'\u001b[0m: \u001b[37m5\u001b[0m, \u001b[90m'num_workers'\u001b[0m: \u001b[37m1\u001b[0m\u001b[1m}\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'input'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'annotation_name'\u001b[0m: \u001b[1m[\u001b[0m\u001b[90m'\u001b[0m\u001b[36mpolygon#annotator1\u001b[0m\u001b[90m'\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'\u001b[0m\u001b[36mpolygon#annotator2\u001b[0m\u001b[90m'\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'\u001b[0m\u001b[36mpolygon#annotator3\u001b[0m\u001b[90m'\u001b[0m\u001b[1m]\u001b[0m,         \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'data_spec'\u001b[0m:                                       \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'/mnt/ssd_8tb/htc/src/tutorials/network_training/pigs_thoraci\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[90mc_2folds.json'\u001b[0m,                                               \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'epoch_size'\u001b[0m: \u001b[37m500\u001b[0m,                                 \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'merge_annotations'\u001b[0m: \u001b[90m'union'\u001b[0m,                      \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'n_channels'\u001b[0m: \u001b[37m100\u001b[0m,                                 \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'preprocessing'\u001b[0m: \u001b[90m'L1'\u001b[0m,                             \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'transforms_gpu'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[90m'class'\u001b[0m: \u001b[90m'KorniaTransform'\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'degrees'\u001b[0m: \u001b[37m45\u001b[0m,                 \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'p'\u001b[0m: \u001b[37m0.5\u001b[0m,                      \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'padding_mode'\u001b[0m: \u001b[90m'reflection'\u001b[0m,  \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'scale'\u001b[0m: \u001b[1m[\u001b[0m\u001b[37m0.9\u001b[0m, \u001b[37m1.1\u001b[0m\u001b[1m]\u001b[0m,           \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'transformation_name'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'RandomAffine'\u001b[0m,                                               \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'translate'\u001b[0m: \u001b[1m[\u001b[0m\u001b[37m0.0625\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      "\u001b[37m0.0625\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,                                                     \u001b[2m                  \u001b[0m\n",
      "                              \u001b[1m{\u001b[0m\u001b[90m'class'\u001b[0m: \u001b[90m'KorniaTransform'\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'p'\u001b[0m: \u001b[37m0.25\u001b[0m,                     \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'transformation_name'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'RandomHorizontalFlip'\u001b[0m\u001b[1m}\u001b[0m,                                      \u001b[2m                  \u001b[0m\n",
      "                              \u001b[1m{\u001b[0m\u001b[90m'class'\u001b[0m: \u001b[90m'KorniaTransform'\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'p'\u001b[0m: \u001b[37m0.25\u001b[0m,                     \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'transformation_name'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'RandomVerticalFlip'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,                                      \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'label_mapping'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'heart'\u001b[0m: \u001b[37m0\u001b[0m, \u001b[90m'lung'\u001b[0m: \u001b[37m1\u001b[0m, \u001b[90m'unlabeled'\u001b[0m: \u001b[37m100\u001b[0m\u001b[1m}\u001b[0m,  \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'lightning_class'\u001b[0m:                                           \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'/mnt/ssd_8tb/htc/src/tutorials/network_training/LightningIma\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[90mgeThoracic.py>LightningImageThoracic'\u001b[0m,                        \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'model'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'architecture_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'encoder_name'\u001b[0m:            \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'efficientnet-b5'\u001b[0m,                                            \u001b[2m                  \u001b[0m\n",
      "                                   \u001b[90m'encoder_weights'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'imagenet'\u001b[0m\u001b[1m}\u001b[0m,                                                  \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'architecture_name'\u001b[0m: \u001b[90m'Unet'\u001b[0m,                       \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'model_name'\u001b[0m: \u001b[90m'ModelImage'\u001b[0m\u001b[1m}\u001b[0m,                       \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'optimization'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'lr_scheduler'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'gamma'\u001b[0m: \u001b[37m0.99\u001b[0m, \u001b[90m'name'\u001b[0m:     \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'ExponentialLR'\u001b[0m\u001b[1m}\u001b[0m,                                             \u001b[2m                  \u001b[0m\n",
      "                  \u001b[90m'optimizer'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'lr'\u001b[0m: \u001b[37m0.001\u001b[0m,                  \u001b[2m                  \u001b[0m\n",
      "                                \u001b[90m'name'\u001b[0m: \u001b[90m'Adam'\u001b[0m,               \u001b[2m                  \u001b[0m\n",
      "                                \u001b[90m'weight_decay'\u001b[0m: \u001b[37m0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'swa_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'annealing_epochs'\u001b[0m: \u001b[37m0\u001b[0m\u001b[1m}\u001b[0m,                       \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'trainer_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'accelerator'\u001b[0m: \u001b[90m'gpu'\u001b[0m,                     \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'devices'\u001b[0m: \u001b[37m1\u001b[0m,                             \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'enable_progress_bar'\u001b[0m: \u001b[3;91mFalse\u001b[0m,             \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'max_epochs'\u001b[0m: \u001b[37m2\u001b[0m,                          \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'precision'\u001b[0m: \u001b[90m'16-mixed'\u001b[0m\u001b[1m}\u001b[0m,                 \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'validation'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'checkpoint_metric'\u001b[0m: \u001b[90m'dice_metric'\u001b[0m,           \u001b[2m                  \u001b[0m\n",
      "                \u001b[90m'checkpoint_metric_mode'\u001b[0m: \u001b[90m'class_level'\u001b[0m,      \u001b[2m                  \u001b[0m\n",
      "                \u001b[90m'dataset_index'\u001b[0m: \u001b[37m0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m                          \u001b[2m                  \u001b[0m\n",
      "Global seed set to 1337\n",
      "\u001b[1m[\u001b[0m\u001b[94mDEBUG\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Used transformations:                             \u001b[2mtransforms.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m121\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[1;35mToType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m                                  \u001b[2m                 \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[94mDEBUG\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Used transformations:                             \u001b[2mtransforms.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m121\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[1;35mToType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m                                  \u001b[2m                 \u001b[0m\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params\n",
      "------------------------------------------------------\n",
      "0 | model            | ModelImage       | 31.3 M\n",
      "1 | ce_loss_weighted | CrossEntropyLoss | 0     \n",
      "2 | dice_loss        | DiceLoss         | 0     \n",
      "3 | focal_loss       | FocalLoss        | 0     \n",
      "------------------------------------------------------\n",
      "31.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.032   Total estimated model params size (MB)\n",
      "Swapping scheduler `ExponentialLR` for `SWALR`\n",
      "\u001b[1m[\u001b[0m\u001b[94mDEBUG\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Used transformations:                             \u001b[2mtransforms.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m121\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[1;35mToType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float32\u001b[1m)\u001b[0m, KorniaTransform\u001b[1m]\u001b[0m                 \u001b[2m                 \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[94mDEBUG\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Used transformations:                             \u001b[2mtransforms.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m121\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[1;35mToType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float32\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m                                  \u001b[2m                 \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Changed check_val_every_n_epoch to \u001b[37m1\u001b[0m and         \u001b[2mHTCLightning.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m184\u001b[0m\n",
      "switched to manual optimization                              \u001b[2m                   \u001b[0m\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Training time for the fold fold_P093: \u001b[37m1\u001b[0m minutes   \u001b[2mrun_training.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m58\u001b[0m\n",
      "and \u001b[37m42.31\u001b[0m seconds                                             \u001b[2m                  \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Peak memory consumption for the fold fold_P093:   \u001b[2mrun_training.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m62\u001b[0m\n",
      "\u001b[37m6.57\u001b[0m GiB                                                      \u001b[2m                  \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Starting training of the fold fold_P094 \u001b[1m[\u001b[0m\u001b[37m2\u001b[0m/\u001b[37m2\u001b[0m\u001b[1m]\u001b[0m    \u001b[2mrun_training.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m265\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The following config will be used for training:   \u001b[2mrun_training.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m70\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m \u001b[1m{\u001b[0m\u001b[90m'config_name'\u001b[0m: \u001b[90m'config_thoracic'\u001b[0m,                \u001b[2mrun_training.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m71\u001b[0m\n",
      " \u001b[90m'dataloader_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'batch_size'\u001b[0m: \u001b[37m5\u001b[0m, \u001b[90m'num_workers'\u001b[0m: \u001b[37m1\u001b[0m\u001b[1m}\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'input'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'annotation_name'\u001b[0m: \u001b[1m[\u001b[0m\u001b[90m'\u001b[0m\u001b[36mpolygon#annotator1\u001b[0m\u001b[90m'\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'\u001b[0m\u001b[36mpolygon#annotator2\u001b[0m\u001b[90m'\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'\u001b[0m\u001b[36mpolygon#annotator3\u001b[0m\u001b[90m'\u001b[0m\u001b[1m]\u001b[0m,         \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'data_spec'\u001b[0m:                                       \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'/mnt/ssd_8tb/htc/src/tutorials/network_training/pigs_thoraci\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[90mc_2folds.json'\u001b[0m,                                               \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'epoch_size'\u001b[0m: \u001b[37m500\u001b[0m,                                 \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'merge_annotations'\u001b[0m: \u001b[90m'union'\u001b[0m,                      \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'n_channels'\u001b[0m: \u001b[37m100\u001b[0m,                                 \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'preprocessing'\u001b[0m: \u001b[90m'L1'\u001b[0m,                             \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'transforms_gpu'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[90m'class'\u001b[0m: \u001b[90m'KorniaTransform'\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'degrees'\u001b[0m: \u001b[37m45\u001b[0m,                 \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'p'\u001b[0m: \u001b[37m0.5\u001b[0m,                      \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'padding_mode'\u001b[0m: \u001b[90m'reflection'\u001b[0m,  \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'scale'\u001b[0m: \u001b[1m[\u001b[0m\u001b[37m0.9\u001b[0m, \u001b[37m1.1\u001b[0m\u001b[1m]\u001b[0m,           \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'transformation_name'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'RandomAffine'\u001b[0m,                                               \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'translate'\u001b[0m: \u001b[1m[\u001b[0m\u001b[37m0.0625\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      "\u001b[37m0.0625\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,                                                     \u001b[2m                  \u001b[0m\n",
      "                              \u001b[1m{\u001b[0m\u001b[90m'class'\u001b[0m: \u001b[90m'KorniaTransform'\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'p'\u001b[0m: \u001b[37m0.25\u001b[0m,                     \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'transformation_name'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'RandomHorizontalFlip'\u001b[0m\u001b[1m}\u001b[0m,                                      \u001b[2m                  \u001b[0m\n",
      "                              \u001b[1m{\u001b[0m\u001b[90m'class'\u001b[0m: \u001b[90m'KorniaTransform'\u001b[0m,    \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'p'\u001b[0m: \u001b[37m0.25\u001b[0m,                     \u001b[2m                  \u001b[0m\n",
      "                               \u001b[90m'transformation_name'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'RandomVerticalFlip'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,                                      \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'label_mapping'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'heart'\u001b[0m: \u001b[37m0\u001b[0m, \u001b[90m'lung'\u001b[0m: \u001b[37m1\u001b[0m, \u001b[90m'unlabeled'\u001b[0m: \u001b[37m100\u001b[0m\u001b[1m}\u001b[0m,  \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'lightning_class'\u001b[0m:                                           \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'/mnt/ssd_8tb/htc/src/tutorials/network_training/LightningIma\u001b[0m \u001b[2m                  \u001b[0m\n",
      "\u001b[90mgeThoracic.py>LightningImageThoracic'\u001b[0m,                        \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'model'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'architecture_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'encoder_name'\u001b[0m:            \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'efficientnet-b5'\u001b[0m,                                            \u001b[2m                  \u001b[0m\n",
      "                                   \u001b[90m'encoder_weights'\u001b[0m:         \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'imagenet'\u001b[0m\u001b[1m}\u001b[0m,                                                  \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'architecture_name'\u001b[0m: \u001b[90m'Unet'\u001b[0m,                       \u001b[2m                  \u001b[0m\n",
      "           \u001b[90m'model_name'\u001b[0m: \u001b[90m'ModelImage'\u001b[0m\u001b[1m}\u001b[0m,                       \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'optimization'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'lr_scheduler'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'gamma'\u001b[0m: \u001b[37m0.99\u001b[0m, \u001b[90m'name'\u001b[0m:     \u001b[2m                  \u001b[0m\n",
      "\u001b[90m'ExponentialLR'\u001b[0m\u001b[1m}\u001b[0m,                                             \u001b[2m                  \u001b[0m\n",
      "                  \u001b[90m'optimizer'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'lr'\u001b[0m: \u001b[37m0.001\u001b[0m,                  \u001b[2m                  \u001b[0m\n",
      "                                \u001b[90m'name'\u001b[0m: \u001b[90m'Adam'\u001b[0m,               \u001b[2m                  \u001b[0m\n",
      "                                \u001b[90m'weight_decay'\u001b[0m: \u001b[37m0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,          \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'swa_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'annealing_epochs'\u001b[0m: \u001b[37m0\u001b[0m\u001b[1m}\u001b[0m,                       \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'trainer_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'accelerator'\u001b[0m: \u001b[90m'gpu'\u001b[0m,                     \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'devices'\u001b[0m: \u001b[37m1\u001b[0m,                             \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'enable_progress_bar'\u001b[0m: \u001b[3;91mFalse\u001b[0m,             \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'max_epochs'\u001b[0m: \u001b[37m2\u001b[0m,                          \u001b[2m                  \u001b[0m\n",
      "                    \u001b[90m'precision'\u001b[0m: \u001b[90m'16-mixed'\u001b[0m\u001b[1m}\u001b[0m,                 \u001b[2m                  \u001b[0m\n",
      " \u001b[90m'validation'\u001b[0m: \u001b[1m{\u001b[0m\u001b[90m'checkpoint_metric'\u001b[0m: \u001b[90m'dice_metric'\u001b[0m,           \u001b[2m                  \u001b[0m\n",
      "                \u001b[90m'checkpoint_metric_mode'\u001b[0m: \u001b[90m'class_level'\u001b[0m,      \u001b[2m                  \u001b[0m\n",
      "                \u001b[90m'dataset_index'\u001b[0m: \u001b[37m0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m                          \u001b[2m                  \u001b[0m\n",
      "Global seed set to 1337\n",
      "\u001b[1m[\u001b[0m\u001b[94mDEBUG\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Used transformations:                             \u001b[2mtransforms.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m121\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[1;35mToType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m                                  \u001b[2m                 \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[94mDEBUG\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Used transformations:                             \u001b[2mtransforms.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m121\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[1;35mToType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m                                  \u001b[2m                 \u001b[0m\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type             | Params\n",
      "------------------------------------------------------\n",
      "0 | model            | ModelImage       | 31.3 M\n",
      "1 | ce_loss_weighted | CrossEntropyLoss | 0     \n",
      "2 | dice_loss        | DiceLoss         | 0     \n",
      "3 | focal_loss       | FocalLoss        | 0     \n",
      "------------------------------------------------------\n",
      "31.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "31.3 M    Total params\n",
      "125.032   Total estimated model params size (MB)\n",
      "Swapping scheduler `ExponentialLR` for `SWALR`\n",
      "\u001b[1m[\u001b[0m\u001b[94mDEBUG\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Used transformations:                             \u001b[2mtransforms.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m121\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[1;35mToType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float32\u001b[1m)\u001b[0m, KorniaTransform\u001b[1m]\u001b[0m                 \u001b[2m                 \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The model ModelImage expects L1 normalized input  \u001b[2mHTCModel.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m186\u001b[0m\n",
      "but the features \u001b[1m(\u001b[0mfeatures.shape = \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[37m5\u001b[0m, \u001b[37m100\u001b[0m, \u001b[37m480\u001b[0m,      \u001b[2m               \u001b[0m\n",
      "\u001b[37m640\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m do not seem to be L1 normalized:                          \u001b[2m               \u001b[0m\n",
      "average per pixel = \u001b[37m0.9999703764915466\u001b[0m                           \u001b[2m               \u001b[0m\n",
      "standard deviation per pixel = \u001b[37m0.0008805753896012902\u001b[0m             \u001b[2m               \u001b[0m\n",
      "This check is only performed for the first batch.                \u001b[2m               \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[94mDEBUG\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Used transformations:                             \u001b[2mtransforms.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m121\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[1;35mToType\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.float32\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m                                  \u001b[2m                 \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Changed check_val_every_n_epoch to \u001b[37m1\u001b[0m and         \u001b[2mHTCLightning.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m184\u001b[0m\n",
      "switched to manual optimization                              \u001b[2m                   \u001b[0m\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Training time for the fold fold_P094: \u001b[37m1\u001b[0m minutes   \u001b[2mrun_training.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m58\u001b[0m\n",
      "and \u001b[37m42.35\u001b[0m seconds                                             \u001b[2m                  \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Peak memory consumption for the fold fold_P094:   \u001b[2mrun_training.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m62\u001b[0m\n",
      "\u001b[37m6.57\u001b[0m GiB                                                      \u001b[2m                  \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Training time for the all folds: \u001b[37m3\u001b[0m minutes and   \u001b[2mrun_training.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m274\u001b[0m\n",
      "\u001b[37m32.74\u001b[0m seconds                                                \u001b[2m                   \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!htc training --model image --config $config_path\n",
    "assert _exit_code == 0, \"Training was not successful\"  # noqa: F821"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddcd7d1-df01-4288-b0e5-dbd2465c8e68",
   "metadata": {},
   "source": [
    "The results will be stored in your results folder (as set by the `PATH_HTC_RESULTS` environment variable). There will be a subfolder for each fold with the following files:\n",
    "- A copy of your configuration file (`config.json`).\n",
    "- A copy of your data specification file (`data.json`).\n",
    "- The trained network (e.g. `epoch=02-dice_metric=0.90.ckpt`).\n",
    "- TensorBoard events file (e.g. `events.out.tfevents.1670877422.e130-pc27.2610059.0`). This file contains the variables you logged via `self.log()`, e.g. the value of our focal loss. It is also possible to start TensorBoard in the results folder and visualize the values while the network is still training (helpful for debugging).\n",
    "- The log messages you may have seen in your console (or in the above cell) during training saved in a text file (`log.txt`).\n",
    "- Statistics about your CPU and GPU utilization during training (e.g. `system_log_2022-12-12_21-37-00.json`).\n",
    "- Statistics about the seen images during training (`trainings_stats.npz`).\n",
    "- A table with the result of the validation metrics like the dice score computed in the validation step (`validation_results.pkl.xz`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bc2e9fa-841b-4b64-b4ab-9439c00d9a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/j562r/htc/results_test/training/image'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P093'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P093/config.json'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P093/data.json'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P093/epoch=02-dice_metric=1.00.ckpt'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P093/events.out.tfevents.1687977747.e130-pc27.15872.0'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P093/hparams.yaml'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P093/log.txt'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P093/system_log_2023-06-28_20-42-27.json'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P093/trainings_stats.npz'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P093/validation_results.pkl.xz'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P094'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P094/config.json'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P094/data.json'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P094/epoch=02-dice_metric=1.00.ckpt'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P094/events.out.tfevents.1687977854.e130-pc27.16216.0'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P094/hparams.yaml'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P094/log.txt'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P094/system_log_2023-06-28_20-44-13.json'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P094/trainings_stats.npz'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/fold_P094/validation_results.pkl.xz'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/2023-06-28_20-42-23_config_thoracic/log.txt'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/log.txt'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P093'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P093/events.out.tfevents.1687977281.e130-pc27.13831.0'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P093/hparams.yaml'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P093/log.txt'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P093/system_log_2023-06-28_20-34-41.json'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P094'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P094/events.out.tfevents.1687977289.e130-pc27.14089.0'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P094/hparams.yaml'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P094/log.txt'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P094/system_log_2023-06-28_20-34-48.json')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(settings.training_dir.rglob(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6fd23-6278-4708-bb37-3e04124e2b5e",
   "metadata": {},
   "source": [
    "## Aggregating and Visualizing\n",
    "After we trained our network, we would like to know wheter it performs well. You could have visualized the metric scores with TensorBoard but this is only a single view on a single fold. What we want is a holistic view on our network performance. This includes aggregation across folds and correctly aggregating with respect to the hierarchy in the data. The later is easy to miss: we cannot simply average our performance scores across all images as the number of images per subject is not always the same (even not in our standardized dataset). What is more, images from one subject are not independent from each other leading to common [pitfalls](https://arxiv.org/abs/2206.01653). Hence, we always first want to aggregate for each subject independently from each other and then across subjects. In the htc framework, the [`MetricAggregation`](../../htc/models/common/MetricAggregation.py) class is responsible for this.\n",
    "\n",
    "So, what we would like to have is a summary of our training run with nice visualizations for our scores which are aggregated while respecting the hierarchy in the data. Here, the general idea is to write one Jupyter Notebook which contains all these aggregations and visualizations for one training run and then use this as a template to generate it for every trained network. The notebook itself is highly task-specific but there are some notebook in this repository which you can use as reference. Especially for this tutorial, there is also a notebook in this folder which we now want to use. To use this as template for our training run, execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcb95067-aa3d-487f-8cee-2a9c34df93f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = tutorial_dir / \"ExperimentAnalysis.ipynb\"\n",
    "assert notebook_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72d183ac-6154-4ec4-8d38-65fa5d16ebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Will generate results for the following  \u001b[2mrun_table_generation.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m451\u001b[0m\n",
      "runs:                                                \u001b[2m                           \u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m                                          \u001b[2mrun_table_generation.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m453\u001b[0m\n",
      "image/\u001b[37m2023\u001b[0m-\u001b[37m06\u001b[0m-28_20-\u001b[37m42\u001b[0m-23_config_thoracic            \u001b[2m                           \u001b[0m\n",
      "\u001b[2K\u001b[36mCheck for necessary files\u001b[0m \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\u001b[1m[\u001b[0m\u001b[33mWARNING\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m The log of the fold                   \u001b[2mrun_table_generation.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m194\u001b[0m\n",
      "\u001b[35m/home/j562r/htc/results_test/training/image/2023-06-\u001b[0m \u001b[2m                           \u001b[0m\n",
      "\u001b[35m28_20-42-23_config_thoracic/\u001b[0m\u001b[95mfold_P094\u001b[0m contains       \u001b[2m                           \u001b[0m\n",
      "warnings                                             \u001b[2m                           \u001b[0m\n",
      "\u001b[2K\u001b[36mCheck for necessary files\u001b[0m \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\n",
      "\u001b[2K\u001b[36mCreate validation table\u001b[0m \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\n",
      "\u001b[2K\u001b[36mCreate test table\u001b[0m \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\n",
      "\u001b[2K\u001b[36mValidate tables\u001b[0m \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\n",
      "\u001b[?25h\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc\u001b[0m\u001b[1m]\u001b[0m Creating notebooks\u001b[33m...\u001b[0m                    \u001b[2mrun_table_generation.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m465\u001b[0m\n",
      "\u001b[2K\u001b[36mGenerate notebooks\u001b[0m \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m  0%\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:00\u001b[0m \u001b[31m?\u001b[0m\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc.no_duplicates\u001b[0m\u001b[1m]\u001b[0m Using the notebook         \u001b[2mrun_table_generation.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m402\u001b[0m\n",
      "\u001b[35m/mnt/ssd_8tb/htc/src/tutorials/network_training/\u001b[0m\u001b[95mExpe\u001b[0m \u001b[2m                           \u001b[0m\n",
      "\u001b[95mrimentAnalysis.ipynb\u001b[0m                                 \u001b[2m                           \u001b[0m\n",
      "\u001b[2K\u001b[36mGenerate notebooks\u001b[0m \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:04\u001b[0m \u001b[31m?\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!htc table_generation --notebook $notebook_path\n",
    "assert _exit_code == 0, \"Table generation was not successful\"  # noqa: F821"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dd91f4-930f-47c6-ac60-b8ad10db85fc",
   "metadata": {},
   "source": [
    "In the run directory for our training run, there are now several new files:\n",
    "- Most importantly, `ExperimentAnalysis.html` which gives you all the promised visualizations from before.\n",
    "- `config.json` and `data.json` for easier access the configuration and data specifcation for the complete training run.\n",
    "- `validation_table.pkl.xz` is a table with all the validation data from all folds (we are using this table in the generated notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d9773e-03bc-43ee-9d3d-093de992d008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/log.txt'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P093'),\n",
       " PosixPath('/home/j562r/htc/results_test/training/image/error_2023-06-28_20-34-37_config_thoracic/running_fold_P094')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_dir = sorted((settings.training_dir / \"image\").iterdir())[-1]\n",
    "run_folder = run_dir.name\n",
    "sorted(run_dir.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711632a-175c-4e61-a89e-e1e83d0ba9ac",
   "metadata": {},
   "source": [
    "> The `table_generation` command iterates over all available run directories and applies your notebook template to all \"empty\" runs. That is, if you want to re-apply a notebook, simply delete the existing `ExperimentAnalysis.html` and `validation_table.pkl.xz` files. They will be re-generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761e2674-7cd3-4fb9-be0e-2db30d1d73b2",
   "metadata": {},
   "source": [
    "## Bonus: Visualizing Network Predictions\n",
    "If you are (not only) interested in the aggregated visualizations but would also get a feeling of the network predictions, then we got you covered. With the following command, you will get a figure for each image in the validation set which visualized the network output together with the network confidence (but uncalibrated!) and the reference segmentation as comparison. The command works on each fold separately. It will load the network for the corresponding fold and then make predictions for all images which are part of the validation set of this fold. This means that not the same network is used for all images.\n",
    "> Adding the `--test` switch will give you the predictions for all images in the test set (by default with network ensembling across the folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7045b02c-e279-4a12-a0c4-a4a5e305dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc.no_duplicates\u001b[0m\u001b[1m]\u001b[0m Found pretrained run in the local      \u001b[2mHTCModel.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m479\u001b[0m\n",
      "results dir at                                                   \u001b[2m               \u001b[0m\n",
      "\u001b[35m/home/j562r/htc/results_test/training/image/\u001b[0m\u001b[95merror_2023-06-28_20-\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[95m34-37_config_thoracic\u001b[0m                                            \u001b[2m               \u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/ssd_8tb/htc/src/htc/model_processing/run_image_figures.py\", line 105, in <module>\n",
      "    runner.start(ValidationPredictor, ImageFigureConsumer)\n",
      "  File \"/mnt/ssd_8tb/htc/src/htc/model_processing/Runner.py\", line 269, in start\n",
      "    consumer = ConsumerClass(\n",
      "  File \"/mnt/ssd_8tb/htc/src/htc/model_processing/run_image_figures.py\", line 22, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"/mnt/ssd_8tb/htc/src/htc/model_processing/ImageConsumer.py\", line 58, in __init__\n",
      "    self.config = Config(self.run_dir / \"config.json\") if config is None else config\n",
      "  File \"/mnt/ssd_8tb/htc/src/htc/utils/Config.py\", line 116, in __init__\n",
      "    assert self.path_config is not None, (\n",
      "AssertionError: Cannot find the config file at None. Please make sure that the file exists at this location\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Figure generation was not successful",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtc image_figures --model image --run-folder $run_folder --hide-progressbar\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _exit_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFigure generation was not successful\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Figure generation was not successful"
     ]
    }
   ],
   "source": [
    "!htc image_figures --model image --run-folder $run_folder --hide-progressbar\n",
    "assert _exit_code == 0, \"Figure generation was not successful\"  # noqa: F821"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047b3289-a540-40b9-b3f2-42dc574be19b",
   "metadata": {},
   "source": [
    "## Performance On the Untouched Test Set\n",
    "So far, we only assessed our network performance on the validation data and this is a good thing. For all your experiments and network comparisons, you should use the validation set. Only at the very end, it is time to analyse the performance on the untouched test set. Let's assume we are finished with our experiments and would like to know about the performance on the test set. For this you can use the following command which will create a `test_table.pkl.xz` in our run directory. This is very similar to our `validation_table.pkl.xz` just with scores on the test set and no per epoch values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d15615-739c-460f-a410-915bf3e9fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!htc tables --model image --run-folder $run_folder --test --metrics DSC\n",
    "assert _exit_code == 0, \"Table generation was not successful\"  # noqa: F821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d87c40-552f-4964-a6ff-288341ae4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle(run_dir / \"test_table.pkl.xz\")\n",
    "config = Config(run_dir / \"config.json\")\n",
    "agg_test = MetricAggregation(df_test, config, metrics=[\"dice_metric\"])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7541d-449b-4c5e-bc6a-09f807713e62",
   "metadata": {},
   "source": [
    "We could now show similar visualizations as before but to keep it simple, we only look at the class scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c1ce7-328d-432f-96e5-2c0aaec4a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_class_scores_figure(agg_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
