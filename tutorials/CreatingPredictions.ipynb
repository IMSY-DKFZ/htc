{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88283e85-2a29-4816-b5ec-a8d15613eb1c",
   "metadata": {},
   "source": [
    "# Creating Predictions\n",
    "If you have some HSI images and want to get predictions from one of our pretrained models (or your own models), then we covered you in this notebook. Predictions can be computed based on a folder with HSI images and the `htc inference` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50cc1a7-971b-40cd-ba89-43c2835257e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from htc import Config, LabelMapping, decompress_file, settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04bfa34-330b-47f7-b032-290db9e4383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = settings.data_dirs[\"HeiPorSPECTRAL\"]\n",
    "output_dir = settings.results_dir / \"predictions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c10e8-cca8-46f9-91e1-3a9f02d55103",
   "metadata": {},
   "source": [
    "> Note: If you want to use your own data and it does not fit into the default structure (e.g. because you have non-Tivita images), they you need to write your [own DataPath class](./CustomDataPath.md) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1997912-0a42-45e2-b170-d2774a4069ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m[\u001b[0m\u001b[32mINFO\u001b[0m\u001b[1m]\u001b[0m\u001b[1m[\u001b[0m\u001b[3mhtc.no_duplicates\u001b[0m\u001b[1m]\u001b[0m Found pretrained run in the local hub  \u001b[2mHTCModel.py\u001b[0m\u001b[2m:\u001b[0m\u001b[2m180\u001b[0m\n",
      "dir at                                                           \u001b[2m               \u001b[0m\n",
      "\u001b[35m/home/j562r/.cache/torch/hub/htc_checkpoints/image/\u001b[0m\u001b[95m2022-02-03_22\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[95m-58-44_generated_default_model_comparison\u001b[0m                        \u001b[2m               \u001b[0m\n",
      "\u001b[2KDataloader \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!htc inference --input-dir $data_dir/subjects/P086/2021_04_15_09_22_02 --output-dir $output_dir --model image --run-folder 2022-02-03_22-58-44_generated_default_model_comparison\n",
    "assert _exit_code == 0, \"Inference was not successful\"  # noqa: F821"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82bc5e-b48b-4aa5-9dbc-ac300e6724f4",
   "metadata": {},
   "source": [
    "This command searches for all HSI images in the given input directory, computes a prediction using the specified trained model (will also be downloaded if not available) and stores the result in the given output directory. You can use any of the pretrained models here.\n",
    "\n",
    "In this example case, there is only one output image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059251ab-f15f-436b-bca5-deeac44a29ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/mnt/ssd_8tb/htc/results_test/predictions/2022-02-03_22-58-44_generated_default_model_comparison'),\n",
       " PosixPath('/mnt/ssd_8tb/htc/results_test/predictions/2022-02-03_22-58-44_generated_default_model_comparison/0202-00118#2021_04_15_09_22_02.blosc'),\n",
       " PosixPath('/mnt/ssd_8tb/htc/results_test/predictions/2022-02-03_22-58-44_generated_default_model_comparison/0202-00118#2021_04_15_09_22_02.html'),\n",
       " PosixPath('/mnt/ssd_8tb/htc/results_test/predictions/2022-02-03_22-58-44_generated_default_model_comparison/config.json')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(output_dir.rglob(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db9ec1-c7c9-493e-b4af-b3d77f90e22e",
   "metadata": {},
   "source": [
    "Per default, this includes the predicted labels (stored in the blosc file), a visualization of the prediction (HTML file) and the config which was used for computing the predictions (which again is based on the config of the trained model). You can open the HTML file directly with any browser. The labels can be read with the `decompress_file()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9a00b0-7bf0-4237-ae5d-2840ecc38cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ..., 12, 12,  0],\n",
       "       [ 0,  0,  0, ..., 12, 12,  0],\n",
       "       [ 0,  0,  0, ..., 12, 12, 12],\n",
       "       ...,\n",
       "       [ 5,  5,  5, ...,  4,  4,  4],\n",
       "       [ 5,  5,  5, ...,  4,  4,  4],\n",
       "       [ 5,  5,  5, ...,  4,  4,  4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = decompress_file(sorted(output_dir.rglob(\"*.blosc\"))[0])\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b0c7e-89fe-403e-99d8-8d349ff24443",
   "metadata": {},
   "source": [
    "The config can, for example, be used to recover the original label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45244a8c-281f-4a29-b1ac-8f304877c581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background: 55531 pixels\n",
      "liver: 26718 pixels\n",
      "colon: 97583 pixels\n",
      "small_bowel: 1090 pixels\n",
      "stomach: 37645 pixels\n",
      "spleen: 42066 pixels\n",
      "omentum: 9688 pixels\n",
      "peritoneum: 20739 pixels\n",
      "skin: 5816 pixels\n",
      "fat_subcutaneous: 10324 pixels\n"
     ]
    }
   ],
   "source": [
    "config = Config(sorted(output_dir.rglob(\"config.json\"))[0])\n",
    "mapping = LabelMapping.from_config(config)\n",
    "\n",
    "for l, c in zip(*np.unique(labels, return_counts=True)):\n",
    "    print(f\"{mapping.index_to_name(l)}: {c} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b30945-dadb-4cf6-9036-b9aee922ad3d",
   "metadata": {},
   "source": [
    "> Note: if you need the softmax values instead of the label indices of the prediction, add the `--predictions-type softmax` switch to `htc inference`. However, be aware that this requires much more disk space (around 17 MiB instead of 2 KiB per image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
